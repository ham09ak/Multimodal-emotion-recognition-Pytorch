{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7E5jTcyShKQ"
      },
      "source": [
        "# upload libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZAkd2XTCGC3",
        "outputId": "061c99ed-22c8-4eb0-d3e7-6effe06093ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 28.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.13.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (4.1.1)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.12.1+cu113)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9ti5tkPQuWj",
        "outputId": "9746e202-ab93-4580-c789-6ab8706ed959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.56.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.39.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwuE6XUvb8Gb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "from tqdm import tqdm \n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import functools\n",
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "import cv2\n",
        "from facenet_pytorch import MTCNN\n",
        "import json\n",
        "from torch import nn, optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import argparse\n",
        "import random\n",
        "import numbers\n",
        "import csv\n",
        "import shutil\n",
        "import sklearn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8pGipW9kW9j"
      },
      "source": [
        "# pre processing and uploading the ravdess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9rsT6JcTW2S",
        "outputId": "01bdb2c7-6b42-445a-b9fd-28389cec398f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpKGjeBgFyTa"
      },
      "outputs": [],
      "source": [
        "#audiofile = 'RAVDESS//Actor_19//03-01-07-02-01-02-19.wav'\n",
        "##preprocess audio files to ensure they are of the same length. if length is less than 3.6 seconds, it is padded with zeros in the end. otherwise, it is equally cropped from \n",
        "##both sides\n",
        "root = '/content/drive/MyDrive/Colab Notebooks/RAVDESS'\n",
        "target_time = 3.6 #sec\n",
        "for actor in os.listdir(root):\n",
        "    for audiofile in os.listdir(os.path.join(root, actor)):\n",
        "        \n",
        "        if not audiofile.endswith('.wav') or 'croppad' in audiofile:\n",
        "            continue\n",
        "\n",
        "        audios = librosa.core.load(os.path.join(root, actor, audiofile), sr=22050)\n",
        "\t\n",
        "        y = audios[0]\n",
        "        sr = audios[1]\n",
        "        target_length = int(sr * target_time)\n",
        "        if len(y) < target_length:\n",
        "            y = np.array(list(y) + [0 for i in range(target_length - len(y))])\n",
        "        else:\n",
        "            remain = len(y) - target_length\n",
        "            y = y[remain//2:-(remain - remain//2)]\n",
        "\t    \n",
        "        sf.write(os.path.join(root, actor, audiofile[:-4]+'_croppad.wav'), y, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jlKENTKGbfW",
        "outputId": "13d8e32c-0509-420b-9eda-c2e14da8f2de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1/24 [03:28<1:20:00, 208.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 2/24 [06:10<1:06:28, 181.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▎        | 3/24 [09:03<1:02:02, 177.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 4/24 [11:40<56:23, 169.18s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 5/24 [14:14<51:56, 164.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 6/24 [17:41<53:34, 178.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 7/24 [20:21<48:49, 172.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 8/24 [22:57<44:33, 167.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 9/24 [26:26<45:05, 180.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 10/24 [28:55<39:48, 170.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 11/24 [31:32<36:06, 166.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 12/24 [34:51<35:14, 176.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 13/24 [37:26<31:07, 169.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 14/24 [40:44<29:45, 178.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 62%|██████▎   | 15/24 [44:16<28:17, 188.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 16/24 [46:47<23:36, 177.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 71%|███████   | 17/24 [51:08<23:37, 202.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 79%|███████▉  | 19/24 [54:43<13:13, 158.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 20/24 [57:18<10:30, 157.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 21/24 [59:44<07:43, 154.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 22/24 [1:02:18<05:09, 154.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 23/24 [1:05:00<02:36, 156.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [1:07:30<00:00, 168.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "mtcnn = MTCNN(image_size=(720, 1280), device=device)\n",
        "\n",
        "#mtcnn.to(device)\n",
        "save_frames = 15\n",
        "input_fps = 30\n",
        "\n",
        "save_length = 3.6 #seconds\n",
        "save_avi = True\n",
        "\n",
        "failed_videos = []\n",
        "\n",
        "select_distributed = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
        "n_processed = 0\n",
        "for sess in tqdm(sorted(os.listdir(root))):   \n",
        "    for filename in os.listdir(os.path.join(root, sess)):\n",
        "           \n",
        "        if filename.endswith('.mp4'):\n",
        "                        \n",
        "            cap = cv2.VideoCapture(os.path.join(root, sess, filename))  \n",
        "            #calculate length in frames\n",
        "            framen = 0\n",
        "            while True:\n",
        "                i,q = cap.read()\n",
        "                if not i:\n",
        "                    break\n",
        "                framen += 1\n",
        "            cap = cv2.VideoCapture(os.path.join(root, sess, filename))\n",
        "\n",
        "            if save_length*input_fps > framen:                    \n",
        "                skip_begin = int((framen - (save_length*input_fps)) // 2)\n",
        "                for i in range(skip_begin):\n",
        "                    _, im = cap.read() \n",
        "                    \n",
        "            framen = int(save_length*input_fps)    \n",
        "            frames_to_select = select_distributed(save_frames,framen)\n",
        "            save_fps = save_frames // (framen // input_fps) \n",
        "            if save_avi:\n",
        "                out = cv2.VideoWriter(os.path.join(root, sess, filename[:-4]+'_facecroppad.avi'),cv2.VideoWriter_fourcc('M','J','P','G'), save_fps, (224,224))\n",
        "\n",
        "            numpy_video = []\n",
        "            success = 0\n",
        "            frame_ctr = 0\n",
        "            \n",
        "            while True: \n",
        "                ret, im = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                if frame_ctr not in frames_to_select:\n",
        "                    frame_ctr += 1\n",
        "                    continue\n",
        "                else:\n",
        "                    frames_to_select.remove(frame_ctr)\n",
        "                    frame_ctr += 1\n",
        "\n",
        "                try:\n",
        "                    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "                except:\n",
        "                    failed_videos.append((sess, i))\n",
        "                    break\n",
        "\t    \n",
        "                temp = im[:,:,-1]\n",
        "                im_rgb = im.copy()\n",
        "                im_rgb[:,:,-1] = im_rgb[:,:,0]\n",
        "                im_rgb[:,:,0] = temp\n",
        "                im_rgb = torch.tensor(im_rgb)\n",
        "                im_rgb = im_rgb.to(device)\n",
        "\n",
        "                bbox = mtcnn.detect(im_rgb)\n",
        "                if bbox[0] is not None:\n",
        "                    bbox = bbox[0][0]\n",
        "                    bbox = [round(x) for x in bbox]\n",
        "                    x1, y1, x2, y2 = bbox\n",
        "                im = im[y1:y2, x1:x2, :]\n",
        "                im = cv2.resize(im, (224,224))\n",
        "                if save_avi:\n",
        "                    out.write(im)\n",
        "                numpy_video.append(im)\n",
        "            if len(frames_to_select) > 0:\n",
        "                for i in range(len(frames_to_select)):\n",
        "                    if save_avi:\n",
        "                        out.write(np.zeros((224,224,3), dtype = np.uint8))\n",
        "                    numpy_video.append(np.zeros((224,224,3), dtype=np.uint8))\n",
        "            if save_avi:\n",
        "                out.release() \n",
        "            np.save(os.path.join(root, sess, filename[:-4]+'_facecroppad.npy'), np.array(numpy_video))\n",
        "            if len(numpy_video) != 15:\n",
        "                print('Error', sess, filename) \n",
        "    n_processed += 1      \n",
        "    with open('processed.txt', 'a') as f:\n",
        "        f.write(sess + '\\n')\n",
        "    print(failed_videos)   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating the annotations.txt that gonna be used later in the training"
      ],
      "metadata": {
        "id": "t1AXWmgJ1_Bg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVJCVNP1FqBm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#n_folds=5\n",
        "#folds =[ [[1,2,3,4],[5,6,7,8],[9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]],[[5,6,7,8],[9,10,11,12],[13,14,15,16,17,18,19,20,21,22,23,24,1,2,3,4]],[[9,10,11,12],[13,14,15,16],[17,18,9,20,21,22,23,24,1,2,3,4,5,6,7,8]],[[13,14,15,16],[17,18,19,20],[21,22,23,24,1,2,3,4,5,6,7,8,9,10,11,12]],[[17,18,19,20],[21,22,23,24],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]]\n",
        "\n",
        "n_folds=1\n",
        "folds = [[[1,2,3,4],[5,6,7,8],[9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]]]\n",
        "for fold in range(n_folds):\n",
        "        fold_ids = folds[fold]\n",
        "        test_ids, val_ids, train_ids = fold_ids\n",
        "\t\n",
        "        #annotation_file = 'annotations_croppad_fold'+str(fold+1)+'.txt'\n",
        "        annotation_file = 'annotations.txt'\n",
        "\t\n",
        "        for i,actor in enumerate(os.listdir(root)):\n",
        "            for video in os.listdir(os.path.join(root, actor)):\n",
        "                if not video.endswith('.npy') or 'croppad' not in video:\n",
        "                    continue\n",
        "                label = str(int(video.split('-')[2]))\n",
        "\t\t     \n",
        "                audio = '03' + video.split('_face')[0][2:] + '_croppad.wav'  \n",
        "                if i in train_ids:\n",
        "                   with open(annotation_file, 'a') as f:\n",
        "                       f.write(os.path.join(root,actor, video) + ';' + os.path.join(root,actor, audio) + ';' + label + ';training' + '\\n')\n",
        "\t\t\n",
        "\n",
        "                elif i in val_ids:\n",
        "                    with open(annotation_file, 'a') as f:\n",
        "                        f.write(os.path.join(root, actor, video) + ';' + os.path.join(root,actor, audio) + ';'+ label + ';validation' + '\\n')\n",
        "\t\t\n",
        "                else:\n",
        "                    with open(annotation_file, 'a') as f:\n",
        "                        f.write(os.path.join(root, actor, video) + ';' + os.path.join(root,actor, audio) + ';'+ label + ';testing' + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data methods"
      ],
      "metadata": {
        "id": "5chPymoQuwpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKpjQyiFFBY_"
      },
      "outputs": [],
      "source": [
        "#Visual branch\n",
        "def video_loader(video_dir_path):\n",
        "    video = np.load(video_dir_path)    \n",
        "    video_data = []\n",
        "    for i in range(np.shape(video)[0]):\n",
        "        video_data.append(Image.fromarray(video[i,:,:,:]))    \n",
        "    return video_data\n",
        "\n",
        "def get_default_video_loader():\n",
        "    return functools.partial(video_loader)\n",
        "#audio branch\n",
        "def load_audio(audiofile, sr):\n",
        "    audios = librosa.core.load(audiofile, sr)\n",
        "    y = audios[0]\n",
        "    return y, sr\n",
        "\n",
        "def get_mfccs(y, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=10)\n",
        "    return mfcc\n",
        "#dataset processing methods\n",
        "def make_dataset(subset, annotation_path):\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        annots = f.readlines()\n",
        "        \n",
        "    dataset = []\n",
        "    for line in annots:\n",
        "        filename, audiofilename, label, trainvaltest = line.split(';')        \n",
        "        if trainvaltest.rstrip() != subset:\n",
        "            continue\n",
        "        \n",
        "        sample = {'video_path': filename,                       \n",
        "                  'audio_path': audiofilename, \n",
        "                  'label': int(label)-1}\n",
        "        dataset.append(sample)\n",
        "    return dataset \n",
        "       \n",
        "class RAVDESS(data.Dataset):\n",
        "    def __init__(self,                 \n",
        "                 annotation_path,\n",
        "                 subset,\n",
        "                 spatial_transform=None,\n",
        "                 get_loader=get_default_video_loader, data_type = 'audiovisual', audio_transform=None):\n",
        "        self.data = make_dataset(subset, annotation_path)\n",
        "        self.spatial_transform = spatial_transform\n",
        "        self.audio_transform=audio_transform\n",
        "        self.loader = get_loader()\n",
        "        self.data_type = data_type \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.data[index]['label']\n",
        "                \n",
        "\n",
        "        if self.data_type == 'video' or self.data_type == 'audiovisual':        \n",
        "            path = self.data[index]['video_path']\n",
        "            clip = self.loader(path)\n",
        "            \n",
        "            if self.spatial_transform is not None:               \n",
        "                self.spatial_transform.randomize_parameters()\n",
        "                clip = [self.spatial_transform(img) for img in clip]            \n",
        "            clip = torch.stack(clip, 0).permute(1, 0, 2, 3) \n",
        "            \n",
        "            if self.data_type == 'video':\n",
        "                return clip, target\n",
        "            \n",
        "        if self.data_type == 'audio' or self.data_type == 'audiovisual':\n",
        "            path = self.data[index]['audio_path']\n",
        "            y, sr = load_audio(path, sr=22050) \n",
        "            \n",
        "            if self.audio_transform is not None:\n",
        "                 self.audio_transform.randomize_parameters()\n",
        "                 y = self.audio_transform(y)     \n",
        "                 \n",
        "            mfcc = get_mfccs(y, sr)            \n",
        "            audio_features = mfcc \n",
        "\n",
        "            if self.data_type == 'audio':\n",
        "                return audio_features, target\n",
        "        if self.data_type == 'audiovisual':\n",
        "            return audio_features, clip, target  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWklDsnetuCQ"
      },
      "outputs": [],
      "source": [
        "def get_training_set(spatial_transform=None, audio_transform=None):\n",
        "    assert dataset in ['RAVDESS'], print('Unsupported dataset: {}'.format(dataset))\n",
        "\n",
        "    if dataset == 'RAVDESS':\n",
        "        training_data = RAVDESS(\n",
        "            annotation_path,\n",
        "            'training',\n",
        "            spatial_transform=spatial_transform, data_type='audiovisual', audio_transform=audio_transform)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def get_validation_set(spatial_transform=None, audio_transform=None):\n",
        "    assert dataset in ['RAVDESS'], print('Unsupported dataset: {}'.format(dataset))\n",
        "\n",
        "    if 'RAVDESS' == 'RAVDESS':\n",
        "        validation_data = RAVDESS(\n",
        "            annotation_path,\n",
        "            'validation',\n",
        "            spatial_transform=spatial_transform, data_type = 'audiovisual', audio_transform=audio_transform)\n",
        "    return validation_data\n",
        "\n",
        "\n",
        "def get_test_set(spatial_transform=None, audio_transform=None):\n",
        "    assert dataset in ['RAVDESS'], print('Unsupported dataset: {}'.format(dataset))\n",
        "    assert test_subset in ['val', 'test']\n",
        "\n",
        "    if test_subset == 'val':\n",
        "        subset = 'validation'\n",
        "    elif test_subset == 'test':\n",
        "        subset = 'testing'\n",
        "    if dataset == 'RAVDESS':\n",
        "        test_data = RAVDESS(\n",
        "            annotation_path,\n",
        "            subset,\n",
        "            spatial_transform=spatial_transform, data_type='audiovisual',audio_transform=audio_transform)\n",
        "    return test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEGJAiAUavP-"
      },
      "source": [
        "transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq5_Qk6xas0Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "    Example:\n",
        "        >>> Compose([\n",
        "        >>>     CenterCrop(10),\n",
        "        >>>     ToTensor(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        for t in self.transforms:\n",
        "            \n",
        "            img = t(img)\n",
        "        return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        for t in self.transforms:\n",
        "            t.randomize_parameters()\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
        "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm_value=255):\n",
        "        self.norm_value = norm_value\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            # handle numpy array\n",
        "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
        "            # backward compatibility\n",
        "            return img.float().div(self.norm_value)\n",
        "\n",
        "        if accimage is not None and isinstance(pic, accimage.Image):\n",
        "            nppic = np.zeros(\n",
        "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
        "            pic.copyto(nppic)\n",
        "            return torch.from_numpy(nppic)\n",
        "\n",
        "        # handle PIL Image\n",
        "        if pic.mode == 'I':\n",
        "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
        "        elif pic.mode == 'I;16':\n",
        "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
        "        else:\n",
        "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
        "        if pic.mode == 'YCbCr':\n",
        "            nchannel = 3\n",
        "        elif pic.mode == 'I;16':\n",
        "            nchannel = 1\n",
        "        else:\n",
        "            nchannel = len(pic.mode)\n",
        "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
        "        # put it from HWC to CHW format\n",
        "        # yikes, this transpose takes 80% of the loading time/CPU\n",
        "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        #print(img.size(), img.float().div(self.norm_value)) \n",
        "        if isinstance(img, torch.ByteTensor):\n",
        "            return img.float().div(self.norm_value)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    \"\"\"Crops the given PIL.Image at the center.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL.Image): Image to be cropped.\n",
        "        Returns:\n",
        "            PIL.Image: Cropped image.\n",
        "        \"\"\"\n",
        "        w, h = img.size\n",
        "        th, tw = self.size\n",
        "        x1 = int(round((w - tw) / 2.))\n",
        "        y1 = int(round((h - th) / 2.))\n",
        "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL.Image): Image to be flipped.\n",
        "        Returns:\n",
        "            PIL.Image: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if self.p < 0.5:\n",
        "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        return img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        self.p = random.random()\n",
        "\n",
        "\n",
        "class RandomRotate(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.interpolation = Image.BILINEAR\n",
        "\n",
        "    def __call__(self, img):\n",
        "        im_size = img.size\n",
        "        ret_img = img.rotate(self.rotate_angle, resample=self.interpolation)\n",
        "\n",
        "        return ret_img\n",
        "\n",
        "    def randomize_parameters(self):\n",
        "        self.rotate_angle = random.randint(-10, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m9p5ztwaZXX"
      },
      "source": [
        "utils for the training and the validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SunfWyMkbrQL"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, path, header):\n",
        "        self.log_file = open(path, 'w')\n",
        "        self.logger = csv.writer(self.log_file, delimiter='\\t')\n",
        "\n",
        "        self.logger.writerow(header)\n",
        "        self.header = header\n",
        "\n",
        "    def __del(self):\n",
        "        self.log_file.close()\n",
        "\n",
        "    def log(self, values):\n",
        "        write_values = []\n",
        "        for col in self.header:\n",
        "            assert col in values\n",
        "            write_values.append(values[col])\n",
        "\n",
        "        self.logger.writerow(write_values)\n",
        "        self.log_file.flush()\n",
        "\n",
        "\n",
        "\n",
        "def calculate_accuracy(output, target, topk=(1,), binary=False):\n",
        "    \"\"\"Computes the accuracy@k for the specified values of k\"\"\"\n",
        "    \n",
        "    maxk = max(topk)\n",
        "    #print('target', target, 'output', output)    \n",
        "    if maxk > output.size(1):\n",
        "        maxk = output.size(1)\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    #print('Target: ', target, 'Pred: ', pred)\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    \n",
        "    res = []\n",
        "    for k in topk:\n",
        "        if k > maxk:\n",
        "            k = maxk\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    if binary:\n",
        "        #print(list(target.cpu().numpy()),  list(pred[0].cpu().numpy()))\n",
        "        f1 = sklearn.metrics.f1_score(list(target.cpu().numpy()),  list(pred[0].cpu().numpy()))\n",
        "        print('F1: ', f1)\n",
        "        return res, f1*100\n",
        "    #print(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, fold):\n",
        "    torch.save(state, '%s/%s_checkpoint'% (result_path, store_name)+str(fold)+'.pth')\n",
        "    if is_best:\n",
        "        shutil.copyfile('%s/%s_checkpoint' % (result_path, store_name)+str(fold)+'.pth','%s/%s_best' % (result_path, store_name)+str(fold)+'.pth')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr_new = 0.04 * (0.1 ** (sum(epoch >= np.array([40, 55, 65, 70, 200, 250]))))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_new\n",
        "        #param_group['lr'] = 0.04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70JNmRZTnYft"
      },
      "source": [
        "#  Multimodal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRVOEwhnes5V"
      },
      "source": [
        "efficientface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsZC2TX4aGpB"
      },
      "outputs": [],
      "source": [
        "def depthwise_conv(i, o, kernel_size, stride=1, padding=0, bias=False):\n",
        "    return nn.Conv2d(i, o, kernel_size, stride, padding, bias=bias, groups=i)\n",
        "\n",
        "\n",
        "def channel_shuffle(x, groups):\n",
        "    batchsize, num_channels, height, width = x.data.size()\n",
        "    channels_per_group = num_channels // groups\n",
        "    # reshape\n",
        "    x = x.view(batchsize, groups, channels_per_group, height, width)\n",
        "    x = torch.transpose(x, 1, 2).contiguous()\n",
        "    # flatten\n",
        "    x = x.view(batchsize, -1, height, width)\n",
        "    return x\n",
        "\n",
        "\n",
        "class LocalFeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, planes, index):\n",
        "        super(LocalFeatureExtractor, self).__init__()\n",
        "        self.index = index\n",
        "\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv1_1 = depthwise_conv(inplanes, planes, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn1_1 = norm_layer(planes)\n",
        "        self.conv1_2 = depthwise_conv(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1_2 = norm_layer(planes)\n",
        "\n",
        "        self.conv2_1 = depthwise_conv(inplanes, planes, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2_1 = norm_layer(planes)\n",
        "        self.conv2_2 = depthwise_conv(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_2 = norm_layer(planes)\n",
        "\n",
        "        self.conv3_1 = depthwise_conv(inplanes, planes, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn3_1 = norm_layer(planes)\n",
        "        self.conv3_2 = depthwise_conv(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_2 = norm_layer(planes)\n",
        "\n",
        "        self.conv4_1 = depthwise_conv(inplanes, planes, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn4_1 = norm_layer(planes)\n",
        "        self.conv4_2 = depthwise_conv(planes, planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_2 = norm_layer(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        patch_11 = x[:, :, 0:28, 0:28]\n",
        "        patch_21 = x[:, :, 28:56, 0:28]\n",
        "        patch_12 = x[:, :, 0:28, 28:56]\n",
        "        patch_22 = x[:, :, 28:56, 28:56]\n",
        "\n",
        "        out_1 = self.conv1_1(patch_11)\n",
        "        out_1 = self.bn1_1(out_1)\n",
        "        out_1 = self.relu(out_1)\n",
        "        out_1 = self.conv1_2(out_1)\n",
        "        out_1 = self.bn1_2(out_1)\n",
        "        out_1 = self.relu(out_1)\n",
        "\n",
        "        out_2 = self.conv2_1(patch_21)\n",
        "        out_2 = self.bn2_1(out_2)\n",
        "        out_2 = self.relu(out_2)\n",
        "        out_2 = self.conv2_2(out_2)\n",
        "        out_2 = self.bn2_2(out_2)\n",
        "        out_2 = self.relu(out_2)\n",
        "\n",
        "        out_3 = self.conv3_1(patch_12)\n",
        "        out_3 = self.bn3_1(out_3)\n",
        "        out_3 = self.relu(out_3)\n",
        "        out_3 = self.conv3_2(out_3)\n",
        "        out_3 = self.bn3_2(out_3)\n",
        "        out_3 = self.relu(out_3)\n",
        "\n",
        "        out_4 = self.conv4_1(patch_22)\n",
        "        out_4 = self.bn4_1(out_4)\n",
        "        out_4 = self.relu(out_4)\n",
        "        out_4 = self.conv4_2(out_4)\n",
        "        out_4 = self.bn4_2(out_4)\n",
        "        out_4 = self.relu(out_4)\n",
        "\n",
        "        out1 = torch.cat([out_1, out_2], dim=2)\n",
        "        out2 = torch.cat([out_3, out_4], dim=2)\n",
        "        out = torch.cat([out1, out2], dim=3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "\n",
        "        if not (1 <= stride <= 3):\n",
        "            raise ValueError('illegal stride value')\n",
        "        self.stride = stride\n",
        "\n",
        "        branch_features = oup // 2\n",
        "        assert (self.stride != 1) or (inp == branch_features << 1)\n",
        "\n",
        "        if self.stride > 1:\n",
        "            self.branch1 = nn.Sequential(\n",
        "                depthwise_conv(inp, inp, kernel_size=3, stride=self.stride, padding=1),\n",
        "                nn.BatchNorm2d(inp),\n",
        "                nn.Conv2d(inp, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(branch_features),\n",
        "                nn.ReLU(inplace=True))\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(inp if (self.stride > 1) else branch_features,\n",
        "                      branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            depthwise_conv(branch_features, branch_features, kernel_size=3, stride=self.stride, padding=1),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.Conv2d(branch_features, branch_features, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(branch_features),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.stride == 1:\n",
        "            x1, x2 = x.chunk(2, dim=1)\n",
        "            out = torch.cat((x1, self.branch2(x2)), dim=1)\n",
        "        else:\n",
        "            out = torch.cat((self.branch1(x), self.branch2(x)), dim=1)\n",
        "\n",
        "        out = channel_shuffle(out, 2)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xta0G5Vpexgp"
      },
      "source": [
        "modulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqtr6Pvle1iT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Channel(nn.Module):\n",
        "    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):\n",
        "        super(Channel, self).__init__()\n",
        "        self.gate_c = nn.Sequential()\n",
        "        self.gate_c.add_module('flatten', Flatten())\n",
        "        gate_channels = [gate_channel]\n",
        "        gate_channels += [gate_channel // reduction_ratio] * num_layers\n",
        "        gate_channels += [gate_channel]\n",
        "        for i in range(len(gate_channels) - 2):\n",
        "            self.gate_c.add_module('gate_c_fc_%d'%i, nn.Linear(gate_channels[i], gate_channels[i+1]))\n",
        "            self.gate_c.add_module('gate_c_bn_%d'%(i+1), nn.BatchNorm1d(gate_channels[i+1]))\n",
        "            self.gate_c.add_module('gate_c_relu_%d'%(i+1), nn.ReLU())\n",
        "        self.gate_c.add_module('gate_c_fc_final', nn.Linear(gate_channels[-2], gate_channels[-1]))\n",
        "\n",
        "    def forward(self, in_tensor):\n",
        "        avg_pool = F.avg_pool2d(in_tensor, in_tensor.size(2), stride=in_tensor.size(2))\n",
        "        return self.gate_c(avg_pool).unsqueeze(2).unsqueeze(3).expand_as(in_tensor)\n",
        "\n",
        "\n",
        "class Spatial(nn.Module):\n",
        "    def __init__(self, gate_channel, reduction_ratio=16, dilation_conv_num=2, dilation_val=4):\n",
        "        super(Spatial, self).__init__()\n",
        "        self.gate_s = nn.Sequential()\n",
        "        self.gate_s.add_module('gate_s_conv_reduce0', nn.Conv2d(gate_channel, gate_channel//reduction_ratio, kernel_size=1))\n",
        "        self.gate_s.add_module('gate_s_bn_reduce0',\tnn.BatchNorm2d(gate_channel//reduction_ratio))\n",
        "        self.gate_s.add_module('gate_s_relu_reduce0', nn.ReLU())\n",
        "        for i in range(dilation_conv_num):\n",
        "            self.gate_s.add_module('gate_s_conv_di_%d'%i, nn.Conv2d(gate_channel//reduction_ratio,\n",
        "                                                                    gate_channel//reduction_ratio,\n",
        "                                                                    kernel_size=3,\n",
        "                                                                    padding=dilation_val,\n",
        "                                                                    dilation=dilation_val))\n",
        "            self.gate_s.add_module('gate_s_bn_di_%d'%i, nn.BatchNorm2d(gate_channel//reduction_ratio) )\n",
        "            self.gate_s.add_module('gate_s_relu_di_%d'%i, nn.ReLU())\n",
        "        self.gate_s.add_module('gate_s_conv_final', nn.Conv2d(gate_channel//reduction_ratio, 1, kernel_size=1))\n",
        "\n",
        "    def forward(self, in_tensor):\n",
        "        return self.gate_s(in_tensor).expand_as(in_tensor)\n",
        "\n",
        "\n",
        "class Modulator(nn.Module):\n",
        "    def __init__(self, gate_channel):\n",
        "        super(Modulator, self).__init__()\n",
        "        self.channel_att = Channel(gate_channel)\n",
        "        self.spatial_att = Spatial(gate_channel)\n",
        "\n",
        "    def forward(self, in_tensor):\n",
        "        att = torch.sigmoid(self.channel_att(in_tensor) * self.spatial_att(in_tensor))\n",
        "        return att * in_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXUG9kvvffni"
      },
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCVO0gdefde8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., use_conv1=False):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.use_conv1 = use_conv1\n",
        "        if use_conv1:\n",
        "            self.fc1 = nn.Conv1d(in_features, hidden_features, kernel_size=3, stride=1,padding='same')\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        if use_conv1:\n",
        "            self.fc2 = nn.Conv1d(hidden_features, out_features, kernel_size=3, stride=1,padding='same')\n",
        "        else:\n",
        "            self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_conv1:\n",
        "            x = x.transpose(1,2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        if self.use_conv1:\n",
        "            x = x.transpose(1,2)\n",
        "        return x\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = out_dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(in_dim_q, out_dim, bias=qkv_bias)\n",
        "        self.kv = nn.Linear(in_dim_k, out_dim * 2, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(out_dim, out_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        self.qkmatrix = None\n",
        "\n",
        "    def forward(self, x, x_q):\n",
        "        B, Nk, Ck = x.shape\n",
        "        B, Nq, Cq = x_q.shape\n",
        "        q = self.q(x_q).reshape(B, Nq, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        kv = self.kv(x).reshape(B, Nk, 2, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        q = q.squeeze(0)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "       \n",
        "        attn = attn.softmax(dim=-1)\n",
        "        \n",
        "        self.qkmatrix = attn\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, Nq, -1)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x, self.qkmatrix\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim_k, in_dim_q, out_dim, num_heads, mlp_ratio=2., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,  use_conv1 = False):\n",
        "        super().__init__()\n",
        "        self.norm1_q = norm_layer(in_dim_q)\n",
        "        self.norm1_k = norm_layer(in_dim_k)\n",
        "        self.attn = Attention(in_dim_k=in_dim_k,in_dim_q=in_dim_q,\n",
        "            out_dim=out_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(out_dim)\n",
        "        mlp_hidden_dim = int(out_dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=out_dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, use_conv1=use_conv1)\n",
        "\n",
        "    def forward(self, xk,xq):\n",
        "        \n",
        "        x, a = self.attn(self.norm1_k(xk), self.norm1_q(xq))\n",
        "        x = self.drop_path(x)\n",
        "        x = x +  self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve69lVEse5x0"
      },
      "source": [
        "multimodalcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kpi0MUre-L_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def conv1d_block(in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "    return nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,stride=stride, padding=padding),nn.BatchNorm1d(out_channels),\n",
        "                                   nn.ReLU(inplace=True)) \n",
        "\n",
        "class EfficientFaceTemporal(nn.Module):\n",
        "\n",
        "    def __init__(self, stages_repeats, stages_out_channels, num_classes=7, im_per_sample=25):\n",
        "        super(EfficientFaceTemporal, self).__init__()\n",
        "\n",
        "        if len(stages_repeats) != 3:\n",
        "            raise ValueError('expected stages_repeats as list of 3 positive ints')\n",
        "        if len(stages_out_channels) != 5:\n",
        "            raise ValueError('expected stages_out_channels as list of 5 positive ints')\n",
        "        self._stage_out_channels = stages_out_channels\n",
        "\n",
        "        input_channels = 3\n",
        "        output_channels = self._stage_out_channels[0]\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n",
        "                                   nn.BatchNorm2d(output_channels),\n",
        "                                   nn.ReLU(inplace=True),)\n",
        "        input_channels = output_channels\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        stage_names = ['stage{}'.format(i) for i in [2, 3, 4]]\n",
        "        for name, repeats, output_channels in zip(stage_names, stages_repeats, self._stage_out_channels[1:]):\n",
        "            seq = [InvertedResidual(input_channels, output_channels, 2)]\n",
        "            for i in range(repeats - 1):\n",
        "                seq.append(InvertedResidual(output_channels, output_channels, 1))\n",
        "            setattr(self, name, nn.Sequential(*seq))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        self.local = LocalFeatureExtractor(29, 116, 1)\n",
        "        self.modulator = Modulator(116)\n",
        "\n",
        "        output_channels = self._stage_out_channels[-1]\n",
        "\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n",
        "                                   nn.BatchNorm2d(output_channels),\n",
        "                                   nn.ReLU(inplace=True),)\n",
        "        self.conv1d_0 = conv1d_block(output_channels, 64)\n",
        "        self.conv1d_1 = conv1d_block(64, 64)\n",
        "        self.conv1d_2 = conv1d_block(64, 128)\n",
        "        self.conv1d_3 = conv1d_block(128, 128)\n",
        "\n",
        "        self.classifier_1 = nn.Sequential(\n",
        "                nn.Linear(128, num_classes),\n",
        "            )\n",
        "        self.im_per_sample = im_per_sample\n",
        "        \n",
        "    def forward_features(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.modulator(self.stage2(x)) + self.local(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = x.mean([2, 3]) #global average pooling\n",
        "        return x\n",
        "\n",
        "    def forward_stage1(self, x):\n",
        "        #Getting samples per batch\n",
        "        assert x.shape[0] % self.im_per_sample == 0, \"Batch size is not a multiple of sequence length.\"\n",
        "        n_samples = x.shape[0] // self.im_per_sample\n",
        "        x = x.view(n_samples, self.im_per_sample, x.shape[1])\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.conv1d_0(x)\n",
        "        x = self.conv1d_1(x)\n",
        "        return x\n",
        "        \n",
        "        \n",
        "    def forward_stage2(self, x):\n",
        "        x = self.conv1d_2(x)\n",
        "        x = self.conv1d_3(x)\n",
        "        return x\n",
        "    \n",
        "    def forward_classifier(self, x):\n",
        "        x = x.mean([-1]) #pooling accross temporal dimension\n",
        "        x1 = self.classifier_1(x)\n",
        "        return x1\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.forward_stage1(x)\n",
        "        x = self.forward_stage2(x)\n",
        "        x = self.forward_classifier(x)\n",
        "        return x\n",
        "        \n",
        "      \n",
        "\n",
        "def init_feature_extractor(model, path):\n",
        "    if path == 'None' or path is None:\n",
        "        return\n",
        "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "    pre_trained_dict = checkpoint['state_dict']\n",
        "    pre_trained_dict = {key.replace(\"module.\", \"\"): value for key, value in pre_trained_dict.items()}\n",
        "    print('Initializing efficientnet')\n",
        "    model.load_state_dict(pre_trained_dict, strict=False)\n",
        "\n",
        "    \n",
        "def get_model(num_classes, task, seq_length):\n",
        "    model = EfficientFaceTemporal([4, 8, 4], [29, 116, 232, 464, 1024], num_classes, task, seq_length)\n",
        "    return model  \n",
        "\n",
        "\n",
        "def conv1d_block_audio(in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "    return nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,stride=stride, padding='valid'),nn.BatchNorm1d(out_channels),\n",
        "                                   nn.ReLU(inplace=True), nn.MaxPool1d(2,1))\n",
        "\n",
        "class AudioCNNPool(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(AudioCNNPool, self).__init__()\n",
        "\n",
        "        input_channels = 10\n",
        "        self.conv1d_0 = conv1d_block_audio(input_channels, 64)\n",
        "        self.conv1d_1 = conv1d_block_audio(64, 128)\n",
        "        self.conv1d_2 = conv1d_block_audio(128, 256)\n",
        "        self.conv1d_3 = conv1d_block_audio(256, 128)\n",
        "        \n",
        "        self.classifier_1 = nn.Sequential(\n",
        "                nn.Linear(128, num_classes),\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.forward_stage1(x)\n",
        "        x = self.forward_stage2(x)\n",
        "        x = self.forward_classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward_stage1(self,x):            \n",
        "        x = self.conv1d_0(x)\n",
        "        x = self.conv1d_1(x)\n",
        "        return x\n",
        "    \n",
        "    def forward_stage2(self,x):\n",
        "        x = self.conv1d_2(x)\n",
        "        x = self.conv1d_3(x)   \n",
        "        return x\n",
        "    \n",
        "    def forward_classifier(self, x):   \n",
        "        x = x.mean([-1]) #pooling accross temporal dimension\n",
        "        x1 = self.classifier_1(x)\n",
        "        return x1\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "class MultiModalCNN(nn.Module):\n",
        "    def __init__(self, num_classes=8, fusion='ia', seq_length=15, pretr_ef='None', num_heads=4):\n",
        "        super(MultiModalCNN, self).__init__()\n",
        "        assert fusion in ['ia', 'it', 'lt'], print('Unsupported fusion method: {}'.format(fusion))\n",
        "\n",
        "        self.audio_model = AudioCNNPool(num_classes=num_classes)\n",
        "        self.visual_model = EfficientFaceTemporal([4, 8, 4], [29, 116, 232, 464, 1024], num_classes, seq_length)\n",
        "\n",
        "        init_feature_extractor(self.visual_model, pretr_ef)\n",
        "                           \n",
        "        e_dim = 128\n",
        "        input_dim_video = 128\n",
        "        input_dim_audio = 128\n",
        "        self.fusion=fusion\n",
        "\n",
        "        if fusion in ['lt', 'it']:\n",
        "            if fusion  == 'lt':\n",
        "                self.av = AttentionBlock(in_dim_k=input_dim_video, in_dim_q=input_dim_audio, out_dim=e_dim, num_heads=num_heads)\n",
        "                self.va = AttentionBlock(in_dim_k=input_dim_audio, in_dim_q=input_dim_video, out_dim=e_dim, num_heads=num_heads)\n",
        "            elif fusion == 'it':\n",
        "                input_dim_video = input_dim_video // 2\n",
        "                self.av1 = AttentionBlock(in_dim_k=input_dim_video, in_dim_q=input_dim_audio, out_dim=input_dim_audio, num_heads=num_heads)\n",
        "                self.va1 = AttentionBlock(in_dim_k=input_dim_audio, in_dim_q=input_dim_video, out_dim=input_dim_video, num_heads=num_heads)   \n",
        "        \n",
        "        elif fusion in ['ia']:\n",
        "            input_dim_video = input_dim_video // 2\n",
        "            \n",
        "            self.av1 = Attention(in_dim_k=input_dim_video, in_dim_q=input_dim_audio, out_dim=input_dim_audio, num_heads=num_heads)\n",
        "            self.va1 = Attention(in_dim_k=input_dim_audio, in_dim_q=input_dim_video, out_dim=input_dim_video, num_heads=num_heads)\n",
        "\n",
        "            \n",
        "        self.classifier_1 = nn.Sequential(\n",
        "                    nn.Linear(e_dim*2, num_classes),\n",
        "                )\n",
        "        \n",
        "            \n",
        "\n",
        "    def forward(self, x_audio, x_visual):\n",
        "\n",
        "        if self.fusion == 'lt':\n",
        "            return self.forward_transformer(x_audio, x_visual)\n",
        "\n",
        "        elif self.fusion == 'ia':\n",
        "            return self.forward_feature_2(x_audio, x_visual)\n",
        "       \n",
        "        elif self.fusion == 'it':\n",
        "            return self.forward_feature_3(x_audio, x_visual)\n",
        "\n",
        " \n",
        "        \n",
        "    def forward_feature_3(self, x_audio, x_visual):\n",
        "        x_audio = self.audio_model.forward_stage1(x_audio)\n",
        "        x_visual = self.visual_model.forward_features(x_visual)\n",
        "        x_visual = self.visual_model.forward_stage1(x_visual)\n",
        "\n",
        "        proj_x_a = x_audio.permute(0,2,1)\n",
        "        proj_x_v = x_visual.permute(0,2,1)\n",
        "\n",
        "        h_av = self.av1(proj_x_v, proj_x_a)\n",
        "        h_va = self.va1(proj_x_a, proj_x_v)\n",
        "        \n",
        "        h_av = h_av.permute(0,2,1)\n",
        "        h_va = h_va.permute(0,2,1)\n",
        "        \n",
        "        x_audio = h_av+x_audio\n",
        "        x_visual = h_va + x_visual\n",
        "\n",
        "        x_audio = self.audio_model.forward_stage2(x_audio)       \n",
        "        x_visual = self.visual_model.forward_stage2(x_visual)\n",
        "        \n",
        "        audio_pooled = x_audio.mean([-1]) #mean accross temporal dimension\n",
        "        video_pooled = x_visual.mean([-1])\n",
        "\n",
        "        x = torch.cat((audio_pooled, video_pooled), dim=-1)\n",
        "        x1 = self.classifier_1(x)\n",
        "        return x1\n",
        "    \n",
        "    def forward_feature_2(self, x_audio, x_visual):\n",
        "        x_audio = self.audio_model.forward_stage1(x_audio)\n",
        "        x_visual = self.visual_model.forward_features(x_visual)\n",
        "        x_visual = self.visual_model.forward_stage1(x_visual)\n",
        "\n",
        "        proj_x_a = x_audio.permute(0,2,1)\n",
        "        proj_x_v = x_visual.permute(0,2,1)\n",
        "\n",
        "        _, h_av = self.av1(proj_x_v, proj_x_a)\n",
        "        _, h_va = self.va1(proj_x_a, proj_x_v)\n",
        "        \n",
        "        if h_av.size(1) > 1: #if more than 1 head, take average\n",
        "            h_av = torch.mean(h_av, axis=1).unsqueeze(1)\n",
        "       \n",
        "        h_av = h_av.sum([-2])\n",
        "\n",
        "        if h_va.size(1) > 1: #if more than 1 head, take average\n",
        "            h_va = torch.mean(h_va, axis=1).unsqueeze(1)\n",
        "\n",
        "        h_va = h_va.sum([-2])\n",
        "\n",
        "        x_audio = h_va*x_audio\n",
        "        x_visual = h_av*x_visual\n",
        "        \n",
        "        x_audio = self.audio_model.forward_stage2(x_audio)       \n",
        "        x_visual = self.visual_model.forward_stage2(x_visual)\n",
        "\n",
        "        audio_pooled = x_audio.mean([-1]) #mean accross temporal dimension\n",
        "        video_pooled = x_visual.mean([-1])\n",
        "        \n",
        "        x = torch.cat((audio_pooled, video_pooled), dim=-1)\n",
        "        \n",
        "        x1 = self.classifier_1(x)\n",
        "        return x1\n",
        "\n",
        "    def forward_transformer(self, x_audio, x_visual):\n",
        "        x_audio = self.audio_model.forward_stage1(x_audio)\n",
        "        proj_x_a = self.audio_model.forward_stage2(x_audio)\n",
        "       \n",
        "        x_visual = self.visual_model.forward_features(x_visual) \n",
        "        x_visual = self.visual_model.forward_stage1(x_visual)\n",
        "        proj_x_v = self.visual_model.forward_stage2(x_visual)\n",
        "           \n",
        "        proj_x_a = proj_x_a.permute(0, 2, 1)\n",
        "        proj_x_v = proj_x_v.permute(0, 2, 1)\n",
        "        h_av = self.av(proj_x_v, proj_x_a)\n",
        "        h_va = self.va(proj_x_a, proj_x_v)\n",
        "       \n",
        "        audio_pooled = h_av.mean([1]) #mean accross temporal dimension\n",
        "        video_pooled = h_va.mean([1])\n",
        "\n",
        "        x = torch.cat((audio_pooled, video_pooled), dim=-1)  \n",
        "        x1 = self.classifier_1(x)\n",
        "        return x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRR8n0qLfOPb"
      },
      "source": [
        "# training the multimodal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJz5Xpwvb0yZ"
      },
      "outputs": [],
      "source": [
        "#TRAIN\n",
        "def train_epoch_multimodal(epoch, data_loader, model, criterion, optimizer, epoch_logger, batch_logger):\n",
        "    print('train at epoch {}'.format(epoch))\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "        \n",
        "    end_time = time.time()\n",
        "    for i, (audio_inputs, visual_inputs, targets) in enumerate(data_loader):\n",
        "        data_time.update(time.time() - end_time)\n",
        "\n",
        "   \n",
        "        targets = targets.to(device)\n",
        "            \n",
        "        if mask is not None:\n",
        "            with torch.no_grad():\n",
        "                \n",
        "                if mask == 'noise':\n",
        "                    audio_inputs = torch.cat((audio_inputs, torch.randn(audio_inputs.size()), audio_inputs), dim=0)                   \n",
        "                    visual_inputs = torch.cat((visual_inputs, visual_inputs, torch.randn(visual_inputs.size())), dim=0) \n",
        "                    targets = torch.cat((targets, targets, targets), dim=0)                    \n",
        "                    shuffle = torch.randperm(audio_inputs.size()[0])\n",
        "                    audio_inputs = audio_inputs[shuffle]\n",
        "                    visual_inputs = visual_inputs[shuffle]\n",
        "                    targets = targets[shuffle]\n",
        "                    \n",
        "                elif mask == 'softhard':\n",
        "                    coefficients = torch.randint(low=0, high=100,size=(audio_inputs.size(0),1,1))/100\n",
        "                    vision_coefficients = 1 - coefficients\n",
        "                    coefficients = coefficients.repeat(1,audio_inputs.size(1),audio_inputs.size(2))\n",
        "                    vision_coefficients = vision_coefficients.unsqueeze(-1).unsqueeze(-1).repeat(1,visual_inputs.size(1), visual_inputs.size(2), visual_inputs.size(3), visual_inputs.size(4))\n",
        "\n",
        "                    audio_inputs = torch.cat((audio_inputs, audio_inputs*coefficients, torch.zeros(audio_inputs.size()), audio_inputs), dim=0) \n",
        "                    visual_inputs = torch.cat((visual_inputs, visual_inputs*vision_coefficients, visual_inputs, torch.zeros(visual_inputs.size())), dim=0)   \n",
        "                    \n",
        "                    targets = torch.cat((targets, targets, targets, targets), dim=0)\n",
        "                    shuffle = torch.randperm(audio_inputs.size()[0])\n",
        "                    audio_inputs = audio_inputs[shuffle]\n",
        "                    visual_inputs = visual_inputs[shuffle]\n",
        "                    targets = targets[shuffle]\n",
        "   \n",
        "  \n",
        "\n",
        "        visual_inputs = visual_inputs.permute(0,2,1,3,4)\n",
        "        visual_inputs = visual_inputs.reshape(visual_inputs.shape[0]*visual_inputs.shape[1], visual_inputs.shape[2], visual_inputs.shape[3], visual_inputs.shape[4])\n",
        "        \n",
        "        audio_inputs = Variable(audio_inputs)\n",
        "        visual_inputs = Variable(visual_inputs)\n",
        "\n",
        "        targets = Variable(targets)\n",
        "        outputs = model(audio_inputs, visual_inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        losses.update(loss.data, audio_inputs.size(0))\n",
        "        acc1, acc5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n",
        "        top1.update(acc1, audio_inputs.size(0))\n",
        "        top5.update(acc5, audio_inputs.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end_time)\n",
        "        end_time = time.time()\n",
        "\n",
        "        batch_logger.log({\n",
        "            'epoch': epoch,\n",
        "            'batch': i+1,\n",
        "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
        "            'loss': losses.val.item(),\n",
        "            'acc1': top1.val.item(),\n",
        "            'acc5': top5.val.item(),\n",
        "            'lr': optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "        if i % 10 ==0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t lr: {lr:.5f}\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'acc {top5.val:.5f} ({top5.avg:.5f})'.format(\n",
        "                      epoch,\n",
        "                      i,\n",
        "                      len(data_loader),\n",
        "                      batch_time=batch_time,\n",
        "                      data_time=data_time,\n",
        "                      loss=losses,\n",
        "                      top5=top5,\n",
        "                      lr=optimizer.param_groups[0]['lr']))\n",
        "\n",
        "    epoch_logger.log({\n",
        "        'epoch': epoch,\n",
        "        'loss': losses.avg.item(),\n",
        "        'acc1': top1.avg.item(),\n",
        "        'acc5': top5.avg.item(),\n",
        "        'lr': optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        " \n",
        "def train_epoch(epoch, data_loader, model, criterion, optimizer, epoch_logger, batch_logger):\n",
        "    print('train at epoch {}'.format(epoch))\n",
        "\n",
        "    if model == 'multimodalcnn':\n",
        "       return train_epoch_multimodal(epoch,  data_loader, model, criterion, optimizer, epoch_logger, batch_logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ2xCZldc-RQ"
      },
      "outputs": [],
      "source": [
        "#validation\n",
        "def val_epoch_multimodal(epoch, data_loader, model, criterion, logger,modality='both',dist=None ):\n",
        "    #for evaluation with single modality, specify which modality to keep and which distortion to apply for the other modaltiy:\n",
        "    \n",
        "    assert modality in ['both', 'audio', 'video']    \n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end_time = time.time()\n",
        "    for i, (inputs_audio, inputs_visual, targets) in enumerate(data_loader):\n",
        "        data_time.update(time.time() - end_time)\n",
        "\n",
        "        if modality == 'audio':\n",
        "            print('Skipping video modality')\n",
        "            if dist == 'zeros':\n",
        "               inputs_visual = torch.zeros(inputs_visual.size())\n",
        "            else:\n",
        "                print('UNKNOWN DIST!')\n",
        "        elif modality == 'video':\n",
        "            print('Skipping audio modality')\n",
        "            if dist == 'zeros':\n",
        "                inputs_audio = torch.zeros(inputs_audio.size())\n",
        "        inputs_visual = inputs_visual.permute(0,2,1,3,4)\n",
        "        inputs_visual = inputs_visual.reshape(inputs_visual.shape[0]*inputs_visual.shape[1], inputs_visual.shape[2], inputs_visual.shape[3], inputs_visual.shape[4])\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        targets = targets.to(device)\n",
        "        with torch.no_grad():\n",
        "            inputs_visual = Variable(inputs_visual)\n",
        "            inputs_audio = Variable(inputs_audio)\n",
        "            targets = Variable(targets)\n",
        "        outputs = model(inputs_audio, inputs_visual)\n",
        "        loss = criterion(outputs, targets)\n",
        "        acc1, acc5 = calculate_accuracy(outputs.data, targets.data, topk=(1,5))\n",
        "        top1.update(acc1, inputs_audio.size(0))\n",
        "        top5.update(acc5, inputs_audio.size(0))\n",
        "\n",
        "        losses.update(loss.data, inputs_audio.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end_time)\n",
        "        end_time = time.time()\n",
        "\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "              'Time {batch_time.val:.5f} ({batch_time.avg:.5f})\\t'\n",
        "              'Data {data_time.val:.5f} ({data_time.avg:.5f})\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'acc {top5.val:.5f} ({top5.avg:.5f})'.format(\n",
        "                  epoch,\n",
        "                  i + 1,\n",
        "                  len(data_loader),\n",
        "                  batch_time=batch_time,\n",
        "                  data_time=data_time,\n",
        "                  loss=losses,\n",
        "                  top5=top5))\n",
        "\n",
        "    logger.log({'epoch': epoch,\n",
        "                'loss': losses.avg.item(),\n",
        "                'acc1': top1.avg.itemt(), \n",
        "                'acc5': top5.avg.item()})\n",
        "\n",
        "    return losses.avg.item(), top5.avg.item()\n",
        "\n",
        "def val_epoch(epoch, data_loader, model, criterion, logger, modality='both', dist=None):\n",
        "    print('validation at epoch {}'.format(epoch))\n",
        "    if model == 'multimodalcnn':\n",
        "       return val_epoch_multimodal(epoch, data_loader, model, criterion, logger, modality, dist=dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMitSqIenR4j"
      },
      "outputs": [],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NID-Netjsf7g"
      },
      "outputs": [],
      "source": [
        "def generate_model(model):\n",
        "    \n",
        "    if model == 'multimodalcnn':   \n",
        "        model = MultiModalCNN(8, fusion, seq_length = 15, pretr_ef=pretrain_path, num_heads=4)\n",
        "\n",
        "    if device != 'cpu':\n",
        "        model = model.to(device)\n",
        "        model = nn.DataParallel(model, device_ids=None)\n",
        "        pytorch_total_params = sum(p.numel() for p in model.parameters() if\n",
        "                               p.requires_grad)\n",
        "        print(\"Total number of trainable parameters: \", pytorch_total_params)\n",
        "        \n",
        "    \n",
        "    return model, model.parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrsE33Xpt-dR"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "no_train=no_val=False\n",
        "dataset='RAVDESS'\n",
        "annotation_path='/content/drive/MyDrive/Colab Notebooks/annotations.txt'\n",
        "test_subset='test'  \n",
        "model='multimodalcnn'\n",
        "pretrain_path='/content/drive/MyDrive/Colab Notebooks/EfficientFace_Trained_on_AffectNet7.pth.tar'\n",
        "store_name='model'\n",
        "result_path = '/content/drive/MyDrive/Colab Notebooks/results'\n",
        "mask='noise'\n",
        "fusion='ia'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYiRZT8WukfW"
      },
      "outputs": [],
      "source": [
        "n_folds = 1\n",
        "test_accuracies = []\n",
        "if not os.path.exists(result_path):\n",
        "   os.makedirs(result_path)\n",
        "\n",
        "arch = '{}'.format(model) \n",
        "pretrained = pretrain_path != 'None'\n",
        "for fold in range(n_folds):\n",
        "\n",
        "    torch.manual_seed(1)\n",
        "    model, parameters = generate_model(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    if not no_train:\n",
        "       video_transform = Compose([\n",
        "       RandomHorizontalFlip(),\n",
        "       RandomRotate(),\n",
        "       ToTensor(255)])\n",
        "\n",
        "       training_data = get_training_set(spatial_transform=video_transform) \n",
        "\n",
        "       train_loader = torch.utils.data.DataLoader(\n",
        "                      training_data,\n",
        "                      batch_size=8,\n",
        "                      shuffle=False,\n",
        "                      num_workers=2,\n",
        "                      pin_memory=True)\n",
        "\t   \n",
        "\n",
        "       train_logger = Logger(os.path.join(result_path, 'train'+str(fold)+'.log'),['epoch', 'loss', 'acc1', 'acc5', 'lr'])\n",
        "       train_batch_logger = Logger(os.path.join(result_path, 'train_batch'+str(fold)+'.log'),['epoch', 'batch', 'iter', 'loss', 'acc1', 'acc5', 'lr'])\n",
        "\n",
        "\n",
        "       optimizer = optim.SGD(\n",
        "                   parameters,\n",
        "                   lr=0.04, \n",
        "                   momentum=0.9,\n",
        "                   dampening=0.9,\n",
        "                   weight_decay=1e-3,\n",
        "                   nesterov=False)\n",
        "       scheduler = lr_scheduler.ReduceLROnPlateau(\n",
        "                   optimizer, 'min', patience=10)\n",
        "\n",
        "    if not no_val:\n",
        "       video_transform = Compose([\n",
        "                         ToTensor(255)]) \n",
        "\n",
        "       validation_data = get_validation_set(spatial_transform=video_transform)\n",
        "\n",
        "       val_loader = torch.utils.data.DataLoader(\n",
        "                    validation_data,\n",
        "                    batch_size=8,\n",
        "                    shuffle=False,\n",
        "                    num_workers=2,\n",
        "                    pin_memory=True)\n",
        "\n",
        "       val_logger = Logger(\n",
        "               os.path.join(result_path, 'val'+str(fold)+'.log'), ['epoch', 'loss',  'acc1', 'acc5'])\n",
        "       test_logger = Logger(\n",
        "               os.path.join(result_path, 'test'+str(fold)+'.log'), ['epoch', 'loss', 'acc1', 'acc5'])\n",
        "\n",
        "    resume_path=''\n",
        "    best_acc = 0\n",
        "    best_loss = 1e10\n",
        "    if resume_path:\n",
        "       print('loading checkpoint {}'.format(resume_path))\n",
        "       checkpoint = torch.load(resume_path)\n",
        "       assert arch == checkpoint['arch']\n",
        "       best_acc = checkpoint['best_acc']\n",
        "       begin_epoch= checkpoint['epoch']\n",
        "       model.load_state_dict(checkpoint['state_dict'])\n",
        "    test=True\n",
        "    n_epochs=100\n",
        "    begin_epoch=1\n",
        "    for i in range(begin_epoch, n_epochs + 1):  # numbers of epochs are 100 + 1\n",
        "\n",
        "        if not no_train:\n",
        "           adjust_learning_rate(optimizer, i)\n",
        "           train_epoch(i, train_loader, model, criterion, optimizer, train_logger, train_batch_logger)\n",
        "           state = {'epoch': i,'arch': arch,\n",
        "                   'state_dict': model.state_dict(),\n",
        "                   'optimizer': optimizer.state_dict(),\n",
        "                   'best_acc': best_acc}\n",
        "           save_checkpoint(state, False, fold)\n",
        "\n",
        "        if not no_val:\n",
        "\n",
        "           validation_loss, acc = val_epoch_multimodal(i, val_loader, model, criterion, val_logger,modality='both',dist=None )\n",
        "           is_best = acc > best_acc\n",
        "           best_acc = max(acc, best_acc)\n",
        "           state = {'epoch': i, 'arch': arch, 'state_dict': model.state_dict(), \n",
        "                    'optimizer': optimizer.state_dict(), 'best_acc': best_acc}\n",
        "   \n",
        "           save_checkpoint(state, is_best, fold)\n",
        "\n",
        "    if test:\n",
        "    \n",
        "       test_logger = Logger(os.path.join(result_path, 'test'+str(fold)+'.log'), ['epoch', 'loss', 'acc1', 'acc5'])\n",
        "       video_transform = Compose([ToTensor(255)])\n",
        "\n",
        "       test_data = get_test_set( spatial_transform=video_transform) \n",
        "\n",
        "     #load best model\n",
        "       best_state = torch.load('%s/%s_best' % (result_path, store_name)+str(fold)+'.pth')\n",
        "       model.load_state_dict(best_state['state_dict'])\n",
        "\n",
        "       test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=False,\n",
        "                num_workers=2, pin_memory=True)\n",
        "\n",
        "       #test_loss, test_acc1 = val_epoch(10000, test_loader, model, criterion, test_logger)\n",
        "       test_loss, test_acc = val_epoch_multimodal(10000, test_loader, model, criterion, test_logger,modality='both',dist=None )\n",
        "       test_accuracies.append(test_acc)\n",
        "print('test accuracy: ' + str(np.mean(np.array(test_accuracies))) +'+'+str(np.std(np.array(test_accuracies))) + '\\n')       "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "T7E5jTcyShKQ",
        "a8pGipW9kW9j",
        "ecDjjlJ8uulh",
        "70JNmRZTnYft",
        "mRR8n0qLfOPb"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}